{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifying whether feedback left on a website is either positive or negative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import sklearn\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 2)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "amazon = pd.read_csv(\"amazon_cells_labelled.txt\",delimiter=\"\\t\",header=None)\n",
    "amazon.columns = ['feedback','score']\n",
    "amazon.head(5)\n",
    "amazon.shape\n",
    "# score 1 (positive), score 0 (negative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import negative sentiment words data to create list\n",
    "# source citation: \n",
    "#    Minqing Hu and Bing Liu. \"Mining and Summarizing Customer Reviews.\" \n",
    "#;       Proceedings of the ACM SIGKDD International Conference on Knowledge \n",
    "#;       Discovery and Data Mining (KDD-2004), Aug 22-25, 2004, Seattle, \n",
    "#;       Washington, USA, \n",
    "neg_words = pd.read_csv(\"negative-words.txt\",delimiter='\\t',encoding=\"ISO-8859-1\",skiprows=34,header=None)\n",
    "neg_words.columns = [\"Negative Words\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feedback</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>So there is no way for me to plug it in here i...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Good case, Excellent value.</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Great for the jawbone.</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            feedback  score\n",
       "0  So there is no way for me to plug it in here i...   True\n",
       "1                        Good case, Excellent value.  False\n",
       "2                             Great for the jawbone.  False"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# change score to boolean values (looking for instances where negative messages return True)\n",
    "amazon['score'] = (amazon['score'] == 0)\n",
    "amazon.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4786"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create list of keywords + \"!\" \n",
    "keywords = list(neg_words.values.flatten())\n",
    "keywords.append(\"no\")\n",
    "keywords.append(\"never\")\n",
    "keywords.append(\"not\")\n",
    "len(keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#strip punctuations from feedback messages\n",
    "def strip_punctuation(message):\n",
    "    from string import punctuation\n",
    "    return ''.join(m for m in message if m not in punctuation)\n",
    "\n",
    "# compare two lists to see if feedback contains negative word\n",
    "def neg_message_check(df,col_name,alist):\n",
    "    import re\n",
    "    message_list = list(df[col_name].values.flatten())\n",
    "    new_message_list = []\n",
    "    for message in message_list:\n",
    "        new_message = strip_punctuation(message.lower())\n",
    "        #escape_message = re.escape(new_message)\n",
    "        new_message_list.append(new_message)\n",
    "        \n",
    "    nm = pd.Series(new_message_list)\n",
    "    df[\"modified_feedback\"] = nm.values\n",
    "    \n",
    "    for key in alist:\n",
    "        escaped_key = re.escape(key)\n",
    "        df[str(key)] = df.modified_feedback.str.contains(\"\" + str(escaped_key) + \"\",case=False)\n",
    "        #amazon[str(key)] = amazon.modified_feedback.apply(lambda sentence: any(word in sentence for word in alist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_message_check(amazon,\"feedback\",keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = amazon[keywords]\n",
    "target = amazon[\"score\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of mislabeled points out of total 1000 points: 218\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import BernoulliNB\n",
    "\n",
    "bnb = BernoulliNB()\n",
    "\n",
    "bnb.fit(data,target)\n",
    "\n",
    "y_pred = bnb.predict(data)\n",
    "\n",
    "print(\"Number of mislabeled points out of total {} points: {}\".format(data.shape[0],(target != y_pred).sum()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier successfully identified negative message: 0.782\n",
      "\n",
      "Classifier failed to identify negative message: 0.218\n"
     ]
    }
   ],
   "source": [
    "# checking accuracy \n",
    "success = (1000-218)/1000\n",
    "fails = 218/1000\n",
    "print(\"Classifier successfully identified negative message: {}\\n\".format(success))\n",
    "print(\"Classifier failed to identify negative message: {}\".format(fails))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[471,  29],\n",
       "       [189, 311]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# confusion matrix (false negative, false positives, sensitivity, specificity)\n",
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix(target,y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "False Negatives (Type II error): 189 of 218 errors are from failing to identify negative messages (miss)\n",
    "\n",
    "False Positives (Type I error): 29 negative messages that were not negative messages (false alarm)\n",
    "\n",
    "Sensitivity : 311 out of 500 (0.622) \"how sensitive model is at identifying positives\"\n",
    "\n",
    "Specificity : 471 out of 500 (0.942) \"% of negatives correctly identified\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing for Overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With 20% Holdout: 0.77\n",
      "Testing on Sample: 0.782\n"
     ]
    }
   ],
   "source": [
    "# holdout grouping\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train,X_test,y_train,y_test = train_test_split(data,target,test_size=0.2,random_state=20)\n",
    "print(\"With 20% Holdout: \" + str(bnb.fit(X_train,y_train).score(X_test,y_test)))\n",
    "print('Testing on Sample: ' + str(bnb.fit(data,target).score(data,target)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Single holdout grouping shows little indication that there's overfitting present in the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.66, 0.77, 0.78, 0.71, 0.79, 0.66, 0.69, 0.75, 0.78, 0.73])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# cross validation (creating several holdout groups)\n",
    "from sklearn.model_selection import cross_val_score\n",
    "cross_val_score(bnb,data,target,cv=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There appears to be some fluctuation in the series of accuracy scores in cross validation array. There could potentially be some overfitting in the data...not clear how much fluctuation is ok to pass overfit test with cross validation.\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
