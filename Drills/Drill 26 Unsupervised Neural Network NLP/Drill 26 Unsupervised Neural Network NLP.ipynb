{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using word2vec to create an unsupervised neural network.\n",
    "\n",
    "Word2vec is a shallow neural network model for converting words to vectors using distributed representation, each word is represented by many neurons, and each neuron is involved in representing many words. \n",
    "\n",
    "Useful for parsing requests written by people, but works well only for larger datasets (i.e. corpus that is several billion words long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import gutenberg, stopwords\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_cleaner(text):\n",
    "    #text = re.sub(\"[\\[].*?[\\]]\", \"\", text)\n",
    "    text = re.sub('[^a-zA-Z]',' ',text)\n",
    "    text = text.lower()\n",
    "    text = text.split()\n",
    "    lmz = WordNetLemmatizer()\n",
    "    text = [lmz.lemmatize(word) for word in text if not word in set(stopwords.words('english'))]\n",
    "    text = ' '.join(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "austen = \"\"\n",
    "for novel in ['persuasion','emma','sense']:\n",
    "    work = gutenberg.raw('austen-'+novel+'.txt')\n",
    "    austen += work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "austen = re.sub(r'Chapter \\d+','',austen)\n",
    "austen = re.sub(r'--',' ',austen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parse the data into sentences\n",
    "austen_sent = nltk.sent_tokenize(austen)\n",
    "#austen_sent = nltk.word_tokenize(austen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = []\n",
    "for sent in list(austen_sent):\n",
    "    cleaned_sentence = text_cleaner(sent)\n",
    "    cleaned_sentence = nltk.word_tokenize(cleaned_sentence)\n",
    "    corpus.append(cleaned_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['one', 'daughter', 'eldest', 'would', 'really', 'given', 'thing', 'much', 'tempted']\n",
      "We have 17565 sentences and 17565 tokens.\n"
     ]
    }
   ],
   "source": [
    "print(corpus[20])\n",
    "print('We have {} sentences and {} tokens.'.format(len(corpus), len(austen_sent)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"loud\" in corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.models import word2vec\n",
    "\n",
    "model = word2vec.Word2Vec(\n",
    "    corpus,\n",
    "    workers=4,     # Number of threads to run in parallel (if your computer does parallel processing).\n",
    "    min_count=10,  # Minimum word count threshold.\n",
    "    window=6,      # Number of words around target word to consider.\n",
    "    sg=0,          # Use CBOW because our corpus is small.\n",
    "    sample=1e-3 ,  # Penalize frequent words.\n",
    "    size=300,      # Word vector length.\n",
    "    hs=1           # Use hierarchical softmax.\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('miss', 0.5413392782211304), ('blame', 0.5381035804748535), ('shew', 0.5361624360084534), ('niece', 0.5323333144187927), ('compare', 0.49518486857414246), ('pressing', 0.4615521728992462), ('mr', 0.45491349697113037), ('handsome', 0.4526069164276123), ('friend', 0.4508165121078491), ('eligible', 0.43741124868392944)]\n",
      "\n",
      " 0.63148165\n",
      "\n",
      " 0.67760015\n",
      "\n",
      " marriage\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/glenn/anaconda3/lib/python3.6/site-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
      "  if np.issubdtype(vec.dtype, np.int):\n",
      "/home/glenn/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:11: DeprecationWarning: Call to deprecated `doesnt_match` (Method will be removed in 4.0.0, use self.wv.doesnt_match() instead).\n",
      "  # This is added back by InteractiveShellApp.init_path()\n"
     ]
    }
   ],
   "source": [
    "# List of words in model.\n",
    "vocab = model.wv.vocab.keys()\n",
    "\n",
    "print(model.wv.most_similar(positive=['lady', 'man'], negative=['woman']))\n",
    "# Similarity is calculated using the cosine, so again 1 is total\n",
    "# similarity and 0 is no similarity.\n",
    "print(\"\\n\",model.wv.similarity('loud', 'aloud'))\n",
    "print(\"\\n\",model.wv.similarity('mr', 'miss'))\n",
    "\n",
    "# One of these things is not like the other...\n",
    "print(\"\\n\",model.doesnt_match(\"breakfast marriage dinner lunch\".split()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
